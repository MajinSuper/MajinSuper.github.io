<!doctype html><html lang="zh-CN"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.24" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.157" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><link rel="icon" type="image/jpg" href="/blogger.jpg"><title>训练与微调 | majin</title><meta name="description" content=""><link rel="preload" href="/assets/style-DDKN9_aW.css" as="style"><link rel="stylesheet" href="/assets/style-DDKN9_aW.css"><link rel="modulepreload" href="/assets/app-KjTT_h7t.js"><link rel="modulepreload" href="/assets/index.html-C5IQ6ARh.js"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-c65f7727><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-bed66c22></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-bed66c22> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-c65f7727 data-v-dc00e0f2><div class="vp-navbar" vp-navbar data-v-dc00e0f2 data-v-eca676eb><div class="wrapper" data-v-eca676eb><div class="container" data-v-eca676eb><div class="title" data-v-eca676eb><div class="vp-navbar-title" data-v-eca676eb data-v-4828c0fc><a class="vp-link link no-icon title" href="/" data-v-4828c0fc><!--[--><!--[--><!--]--><!----><span data-v-4828c0fc>majin</span><!--[--><!--]--><!--]--><!----></a></div></div><div class="content" data-v-eca676eb><div class="content-body" data-v-eca676eb><!--[--><!--]--><div class="vp-navbar-search search" data-v-eca676eb><div class="search-wrapper" data-v-cb3e0943><!----><div id="local-search" data-v-cb3e0943><button type="button" class="mini-search mini-search-button" aria-label="搜索文档" data-v-cb3e0943><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">搜索文档</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-eca676eb data-v-a86d55ed><span id="main-nav-aria-label" class="visually-hidden" data-v-a86d55ed>Main Navigation</span><!--[--><!--[--><a class="vp-link link navbar-menu-link" href="/" tabindex="0" data-v-a86d55ed data-v-7a6bcf52><!--[--><!----><span data-v-7a6bcf52>首页</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/blog/" tabindex="0" data-v-a86d55ed data-v-7a6bcf52><!--[--><!----><span data-v-7a6bcf52>博客</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/blog/tags/" tabindex="0" data-v-a86d55ed data-v-7a6bcf52><!--[--><!----><span data-v-7a6bcf52>标签</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/blog/archives/" tabindex="0" data-v-a86d55ed data-v-7a6bcf52><!--[--><!----><span data-v-7a6bcf52>归档</span><!----><!--]--><!----></a><!--]--><!--]--></nav><!--[--><!--]--><!----><div class="vp-navbar-appearance appearance" data-v-eca676eb data-v-c8c80ec6><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-c8c80ec6 data-v-d35535d4 data-v-d395b6c4><span class="check" data-v-d395b6c4><span class="icon" data-v-d395b6c4><!--[--><span class="vpi-sun sun" data-v-d35535d4></span><span class="vpi-moon moon" data-v-d35535d4></span><!--]--></span></span></button></div><div class="vp-social-links vp-navbar-social-links social-links" data-v-eca676eb data-v-ae0696b4 data-v-3c959de6><!--[--><a class="vp-social-link no-icon" href="/" aria-label="github" target="_blank" rel="noopener" data-v-3c959de6 data-v-eebc891d><span class="vpi-social-github" /></a><!--]--></div><div class="vp-flyout vp-navbar-extra extra" data-v-eca676eb data-v-d73459e2 data-v-97091a75><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-97091a75><span class="vpi-more-horizontal icon" data-v-97091a75></span></button><div class="menu" data-v-97091a75><div class="vp-menu" data-v-97091a75 data-v-7d8c6914><!----><!--[--><!--[--><!----><div class="group" data-v-d73459e2><div class="item appearance" data-v-d73459e2><p class="label" data-v-d73459e2>外观</p><div class="appearance-action" data-v-d73459e2><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-d73459e2 data-v-d35535d4 data-v-d395b6c4><span class="check" data-v-d395b6c4><span class="icon" data-v-d395b6c4><!--[--><span class="vpi-sun sun" data-v-d35535d4></span><span class="vpi-moon moon" data-v-d35535d4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-d73459e2><div class="item social-links" data-v-d73459e2><div class="vp-social-links social-links-list" data-v-d73459e2 data-v-3c959de6><!--[--><a class="vp-social-link no-icon" href="/" aria-label="github" target="_blank" rel="noopener" data-v-3c959de6 data-v-eebc891d><span class="vpi-social-github" /></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-eca676eb data-v-e737b135><span class="container" data-v-e737b135><span class="top" data-v-e737b135></span><span class="middle" data-v-e737b135></span><span class="bottom" data-v-e737b135></span></span></button></div></div></div></div><div class="divider" data-v-eca676eb><div class="divider-line" data-v-eca676eb></div></div></div><!----></header><div class="vp-local-nav fixed reached-top is-blog" data-v-c65f7727 data-v-6e03a609><button class="hidden menu" disabled aria-expanded="false" aria-controls="SidebarNav" data-v-6e03a609><span class="vpi-align-left menu-icon" data-v-6e03a609></span><span class="menu-text" data-v-6e03a609>Menu</span></button><div class="vp-local-nav-outline-dropdown" style="--vp-vh:0px;" data-v-6e03a609 data-v-d75301e9><button data-v-d75301e9>返回顶部</button><!----></div></div><!----><!--[--><div id="VPContent" vp-content class="vp-content" data-v-c65f7727 data-v-f95c1c5f><div class="vp-doc-container is-blog" data-v-f95c1c5f data-v-00837af9><!--[--><!--]--><div class="container" data-v-00837af9><!----><div class="content" data-v-00837af9><div class="content-container" data-v-00837af9><!--[--><!--]--><main class="main" data-v-00837af9><nav class="vp-breadcrumb" data-v-00837af9 data-v-d7544bfb><ol vocab="https://schema.org/" typeof="BreadcrumbList" data-v-d7544bfb><!--[--><li property="itemListElement" typeof="ListItem" data-v-d7544bfb><a class="vp-link link breadcrumb" href="/" property="item" typeof="WebPage" data-v-d7544bfb><!--[-->首页<!--]--><!----></a><span class="vpi-chevron-right" data-v-d7544bfb></span><meta property="name" content="首页" data-v-d7544bfb><meta property="position" content="1" data-v-d7544bfb></li><li property="itemListElement" typeof="ListItem" data-v-d7544bfb><a class="vp-link link breadcrumb" href="/blog/" property="item" typeof="WebPage" data-v-d7544bfb><!--[-->博客<!--]--><!----></a><span class="vpi-chevron-right" data-v-d7544bfb></span><meta property="name" content="博客" data-v-d7544bfb><meta property="position" content="2" data-v-d7544bfb></li><li property="itemListElement" typeof="ListItem" data-v-d7544bfb><a class="vp-link link breadcrumb" href="/blog/categories/?id=e6d14b" property="item" typeof="WebPage" data-v-d7544bfb><!--[-->LLM微调<!--]--><!----></a><span class="vpi-chevron-right" data-v-d7544bfb></span><meta property="name" content="LLM微调" data-v-d7544bfb><meta property="position" content="3" data-v-d7544bfb></li><li property="itemListElement" typeof="ListItem" data-v-d7544bfb><a class="vp-link link breadcrumb current" href="/article/finetuning/" property="item" typeof="WebPage" data-v-d7544bfb><!--[-->训练与微调<!--]--><!----></a><!----><meta property="name" content="训练与微调" data-v-d7544bfb><meta property="position" content="4" data-v-d7544bfb></li><!--]--></ol></nav><!--[--><!--]--><!--[--><h1 class="vp-doc-title page-title" data-v-d1b68b8b>训练与微调 <!----></h1><div class="vp-doc-meta" data-v-d1b68b8b><!--[--><!--]--><p class="reading-time" data-v-d1b68b8b><span class="vpi-books icon" data-v-d1b68b8b></span><span data-v-d1b68b8b>约 1290 字</span><span data-v-d1b68b8b>大约 4 分钟</span></p><p data-v-d1b68b8b><span class="vpi-tag icon" data-v-d1b68b8b></span><!--[--><a class="vp-link link tag vp-tag-j17k" href="/blog/tags/?tag=LLM" data-v-d1b68b8b><!--[-->LLM<!--]--><!----></a><a class="vp-link link tag vp-tag-j17k" href="/blog/tags/?tag=微调" data-v-d1b68b8b><!--[-->微调<!--]--><!----></a><!--]--></p><!--[--><!--]--><p class="create-time" data-v-d1b68b8b><span class="vpi-clock icon" data-v-d1b68b8b></span><span data-v-d1b68b8b>2025-09-20</span></p></div><!--]--><!--[--><!--]--><div class="_article_finetuning_ external-link-icon-enabled vp-doc plume-content" vp-content data-v-00837af9><!--[--><!--]--><div data-v-00837af9><h2 id="训练与微调" tabindex="-1"><a class="header-anchor" href="#训练与微调"><span>训练与微调</span></a></h2><h3 id="基本概念" tabindex="-1"><a class="header-anchor" href="#基本概念"><span>基本概念</span></a></h3><p>微调（Fine-tuning）是指在已经预训练好的大模型基础上，利用<mark>特定领域或任务的数据</mark>对模型进行再次训练的过程。</p><p><mark>主要目的</mark>是让模型更好地适应特定应用场景，提高在<mark>特定任务上的表现</mark>。通过微调，可以让通用大模型具备更强的专业能力、理解特定领域的术语和知识，或者更好地满足用户的个性化需求。</p><div class="hint-container tip"><p class="hint-container-title">常见面试题</p></div><details class="hint-container details"><summary>问题1. 微调和RAG的区别,什么时候用微调？什么时候用RAG？</summary><p>基本区别：</p><ul><li>**微调（Fine-tuning）**是在已有的大模型上，针对<mark>领域数据或领域任务</mark>，对模型参数进一步训练。<mark>模型参数会发生改变</mark>，能<mark>让模型&quot;长期&quot;具备某种能力或知识</mark>。</li><li>**RAG（Retrieval-Augmented Generation）**是一种<mark>将检索和LLM结合</mark>的技术。在<mark>不改变已有的模型参数</mark>基础上，“外挂”知识库，增强模型能力。</li></ul><table><thead><tr><th>维度</th><th>微调</th><th>RAG</th></tr></thead><tbody><tr><td><strong>1.性能成本</strong></td><td>训练代价高，推理成本小</td><td>无训练代价，推理时高</td></tr><tr><td><strong>【1延申】更新频率</strong></td><td>更新慢</td><td>更新快，甚至可以实时</td></tr><tr><td><strong>【1延申】部署场景</strong></td><td>要求时延低</td><td>时延不紧张</td></tr><tr><td><strong>2.效果依赖</strong></td><td>微调的数据质量和覆盖度</td><td>知识库的质量和检索质量</td></tr><tr><td><strong>3.实际场景</strong></td><td>客服（业务逻辑和特定话术）<code>&lt;br&gt;</code> 代码助手（特定语言和框架）<code>&lt;br&gt;</code> 医疗大模型、法律大模型……</td><td>企业知识问答（内部知识库）<code>&lt;br&gt;</code> 学术助手（论文检索和引用） <code>&lt;br&gt;</code> 新闻助手、舆论问答等实时类场景</td></tr><tr><td><strong>4.使用时机</strong></td><td>让具备的风格话术、专业术语<code>&lt;br&gt;</code>知识变化不频繁，可定期更新<code>&lt;br&gt;</code>时延要求低</td><td>知识更新频繁、甚至实时<code>&lt;br&gt;</code>知识库私有或商业机密<code>&lt;br&gt;</code>答案有依据，知识可溯源</td></tr></tbody></table><p><strong>总结</strong>：<mark>微调是&quot;改造&quot;模型本身</mark>，<mark>RAG是&quot;外挂&quot;知识库</mark>。微调适合<mark>模型能力长期固化</mark>，RAG适合<mark>灵活扩展和实时更新知识</mark>。</p></details><details class="hint-container details"><summary>问题2：微调时，全量微调和高效微调有什么区别？各自在什么时机用？</summary><p>区别：</p><ul><li>**全量微调（Full Fine-tuning）**更新所有的模型参数，需要的数据量大、训练时间长，但效果往往比较好</li><li>**高效微调（Parameter-Efficient Fine-tuning, PEFT）**更新少量的参数、训练快、效率高，效果也能接近全量微调。</li></ul><table><thead><tr><th>维度</th><th>全量微调</th><th>高效微调</th></tr></thead><tbody><tr><td><strong>参数量</strong></td><td>所有参数</td><td>少部分参数(1-10%)</td></tr><tr><td><strong>效果</strong></td><td>通常最好</td><td>接近全量微调</td></tr><tr><td><strong>训练速度</strong></td><td>慢</td><td>快</td></tr><tr><td><strong>数据需求</strong></td><td>大量高质量数据</td><td>少量高质量数据</td></tr><tr><td><strong>时机</strong></td><td>数据、训练资源充足<code>&lt;br&gt;</code> 效果精度要求高 <code>&lt;br&gt;</code> 部署后很少更新</td><td>数据有限<code>&lt;br&gt;</code>快速迭代</td></tr><tr><td>:::</td><td></td><td></td></tr></tbody></table><details class="hint-container details"><summary>问题3：PEFT分为哪几类？每一类有哪些代表方法？</summary><p><img src="/images/PEFT_methods.png" alt="PEFT_methods.png"></p><ul><li><strong>选择性方法</strong>：<mark>冻结大部分参数或层</mark>，<mark><strong>选择性</strong>只微调某些参数或者层</mark>。如：BitFit只调偏置项，freeze是冻结大部分层、只调少量层。</li><li><strong>加性方法</strong>：<mark><strong>新增</strong>少量参数或者层</mark>，<mark>只调新增的部分</mark>。如：Prompt-Tuning添加软提示向量、Adapter添加小提示层、Prefix-Tuning、P-Tuning。</li><li><strong>重参数化</strong>：通过对模型的<mark>权重矩阵进行低秩分解</mark>，减少需要更新的参数。如：LoRA矩阵分解来减少更新的参数量。 :::</li></ul><details class="hint-container details"><summary>问题4：讲一下如何从头微调一个LLM，至上线并部署应用</summary><p>step1: 选择模型，根据任务复杂度和算力情况（1B用多大模型？7B用多大模型？） step2：选择数据，收集高质量业务数据（收集多少呢？1B用多少？7B用多少） step3：选择框架，peft、魔搭社区 swift、ansloss step4：训练 step5：评估调优 step6：部署上线</p></details></details></details><h3 id="peft" tabindex="-1"><a class="header-anchor" href="#peft"><span>PEFT</span></a></h3><p><img src="/images/PEFT_methods.png" alt="PEFT_methods.png"></p><ul><li><strong>选择性方法</strong>：<mark>冻结大部分参数或层</mark>，<mark><strong>选择性</strong>只微调某些参数或者层</mark>。如：BitFit（只调偏置项），freeze（冻结大部分层、只调少量层）。</li><li><strong>加性方法</strong>：<mark><strong>新增</strong>少量参数或者层</mark>，<mark>只调新增的部分</mark>。如：Prompt-Tuning（添加软提示向量）、Adapter（添加小提示层）、Prefix-Tuning、P-Tuning。</li><li><strong>重参数化</strong>：通过对模型的<mark>权重矩阵进行低秩分解</mark>，减少需要更新的参数。如：LoRA（矩阵分解来减少更新的参数量）。</li></ul><h4 id="一、选择性方法" tabindex="-1"><a class="header-anchor" href="#一、选择性方法"><span>一、选择性方法</span></a></h4><h5 id="_1-1-freeze" tabindex="-1"><a class="header-anchor" href="#_1-1-freeze"><span>1.1 Freeze</span></a></h5><p><strong>基本原理</strong>：冻结大部分层，只对<mark>少部分层</mark>的参数进行微调。一般，选择的是最后几层。</p><p><strong>优点</strong>：节省显存，训练速度快，数据依赖少。大部分参数固定，模型大部分能力会保留，很少直接会崩坏掉。 <strong>缺点</strong>：选择固定哪几层，有很强的主观性。选择的多少和好坏会直接影响微调效果。于是，也出现了一些评估参数重要性的工作（DiffPruning、FishMask、FAR）</p><h3 id="lora" tabindex="-1"><a class="header-anchor" href="#lora"><span>LORA</span></a></h3><h3 id="qlora" tabindex="-1"><a class="header-anchor" href="#qlora"><span>QLORA</span></a></h3></div><!----><!----><!----></div></main><footer class="vp-doc-footer" data-v-00837af9 data-v-6c6ea141><!--[--><!--]--><!----><div class="contributors" aria-label="Contributors" data-v-6c6ea141><span class="contributors-label" data-v-6c6ea141>贡献者: </span><span class="contributors-info" data-v-6c6ea141><!--[--><!--[--><span class="contributor" data-v-6c6ea141>MajinSuper</span><!--[-->, <!--]--><!--]--><!--[--><span class="contributor" data-v-6c6ea141>majinsuper</span><!----><!--]--><!--]--></span></div><nav class="prev-next" data-v-6c6ea141><div class="pager" data-v-6c6ea141><a class="vp-link link pager-link prev" href="/article/introduce_mcp/" data-v-6c6ea141><!--[--><span class="desc" data-v-6c6ea141>上一页</span><span class="title" data-v-6c6ea141>Agent架构</span><!--]--><!----></a></div><div class="pager" data-v-6c6ea141><a class="vp-link link pager-link next" href="/article/transformer_by_hand/" data-v-6c6ea141><!--[--><span class="desc" data-v-6c6ea141>下一页</span><span class="title" data-v-6c6ea141>手推transformer</span><!--]--><!----></a></div></nav></footer><!----><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button type="button" class="vp-back-to-top" aria-label="back to top" data-v-c65f7727 style="display:none;" data-v-fc53d5da><span class="percent" data-allow-mismatch data-v-fc53d5da>0%</span><span class="show icon vpi-back-to-top" data-v-fc53d5da></span><svg aria-hidden="true" data-v-fc53d5da><circle cx="50%" cy="50%" data-allow-mismatch style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-fc53d5da></circle></svg></button><footer class="vp-footer" vp-footer data-v-c65f7727 data-v-20b8cfc9><!--[--><div class="container" data-v-20b8cfc9><!----><!----></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/assets/app-KjTT_h7t.js" defer></script></body></html>