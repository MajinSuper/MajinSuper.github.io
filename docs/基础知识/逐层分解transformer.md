---
title: é€å±‚åˆ†è§£transformer
tags:
  - transformer
  - åŸºç¡€
createTime: 2025-08-10 09:44:31
permalink: /article/transformer_all_in_one/
---

## 1.ä»‹ç»

### 1.1 æ•´ä½“æ¶æ„

::: center
![transformerä»‹ç».png](/images/transformerä»‹ç».png)
:::

**transformeræ¶æ„**å¦‚ä¸Šå¦‚å›¾æ‰€ç¤ºï¼Œæ˜¯==seq2seqçš„ç½‘ç»œç»“æ„ï¼ˆåºåˆ—è½¬å½•ã€sequence transductionï¼‰==ï¼Œæ˜¯ç”±==Encoder==å’Œ==Decoder==ä¸¤éƒ¨åˆ†ç»„æˆã€‚

::: tip å¸¸è§é¢è¯•é¢˜
:::

::: details é—®é¢˜1. transformerä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼ŸRNNé‡åˆ°äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ
å›ç­”ï¼š
RNNå­˜åœ¨å¹¶è¡Œè®¡ç®—å·®ã€é—å¿˜ç°è±¡ä¸¥é‡ä¸¤ä¸ªä¸»è¦é—®é¢˜
- ==å¹¶è¡Œè®¡ç®—å·®==ï¼šå¤„ç†tæ—¶åˆ»çš„è¾“å…¥ï¼Œéœ€è¦å…ˆå®Œæˆt-1åŠå…¶ä¹‹å‰çš„æ‰€æœ‰è¾“å…¥ã€‚
- ==é—å¿˜ç°è±¡ä¸¥é‡==: å½“è¾“å…¥é•¿åº¦å¾ˆé•¿æ—¶ï¼Œä»…é $h_t$æ¥å­˜å‚¨tæ—¶åˆ»åŠä¹‹å‰çš„ä¿¡æ¯ï¼Œå¾ˆå®¹æ˜“å‡ºç°é—å¿˜ã€‚å³ä¾¿$h_t$ç”¨äº†å¾ˆå¤§çš„sizeã€‚

tansformeråˆå¿«åˆå¥½ï¼š
- ==å¹¶è¡Œè®¡ç®—==ï¼šEncoderå¯å¹¶è¡ŒåŒ–è®¡ç®—æ‰€æœ‰attentionï¼ŒDecoderåœ¨è®­ç»ƒé˜¶æ®µä¹Ÿæ˜¯å¹¶è¡ŒåŒ–çš„ã€‚
- ==æ•ˆæœä¸Šæ›´å¥½==ï¼šåœ¨åŸæ–‡çš„æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šï¼Œæ˜¯å–å¾—äº†éå¸¸å¥½çš„æ•ˆæœã€‚ç›®å‰åœ¨æ›´å¤šå·¥ä½œä¸Šï¼Œä¹ŸéªŒè¯äº†transformerçš„ä¼˜è¶Šæ€§ã€‚
:::

::: details é—®é¢˜2. transformerä¸ºä»€ä¹ˆå¯ä»¥å¹¶è¡ŒåŒ–ï¼Ÿ
å›ç­”ï¼š
è¿™é‡Œï¼Œå¹¶è¡ŒåŒ–æ˜¯æŒ‡æŸæ¡==æ ·æœ¬å†…==æ‰€æœ‰è¾“å…¥ï¼Œå¹¶è¡ŒåŒ–è®¡ç®—ã€‚

**Encoderéƒ¨åˆ†ï¼š**
- QKVï¼šQKVçŸ©é˜µçš„è·å–ï¼Œæ˜¯åŒæ—¶è·å–çš„ï¼Œä¸€æ¬¡é’ˆå¯¹ï¼ˆæ ·æœ¬å†…çš„ï¼‰æ‰€æœ‰è¾“å…¥
- Attnï¼šattentionçŸ©é˜µè®¡ç®—æ˜¯å¹¶è¡Œçš„ï¼Œä¸€æ¬¡é’ˆå¯¹æ‰€æœ‰è¾“å…¥
- MHAï¼šå¤šå¤´ä¹‹é—´ä¹Ÿæ˜¯å¹¶è¡Œçš„
- FNNï¼šä¹Ÿæ˜¯ä¸€æ¬¡é’ˆå¯¹æ‰€æœ‰è¾“å…¥

**Decoderéƒ¨åˆ†ï¼š**
- ==è®­ç»ƒé˜¶æ®µ==ï¼šå¼•å…¥äº†"teacher force"çš„æ¦‚å¿µï¼Œæ¯ä¸ªæ—¶åˆ»==ä¸ä¾èµ–ä¸Šä¸€æ—¶åˆ»çš„è¾“å‡º==ï¼Œè€Œ==ä¾èµ–æ­£ç¡®æ ·æœ¬==ã€‚
- ==æµ‹è¯•é˜¶æ®µ==ï¼šåœ¨æµ‹è¯•é˜¶æ®µï¼Œä¸å­˜åœ¨çœŸå®labelï¼Œé‡‡ç”¨==è‡ªå›å½’==çš„æ–¹å¼ï¼Œè¿˜æ˜¯==ä¾èµ–ä¸Šä¸€æ—¶åˆ»çš„è¾“å‡º==ã€‚
:::

  
### 1.2 åŸºæœ¬æµç¨‹

**è®­ç»ƒæµç¨‹ï¼š**
1. **åˆ†è¯**ï¼šå¯¹ä¸¤æ®µæ–‡æœ¬ï¼Œåˆ†åˆ«è¿›è¡Œåˆ†è¯: 
 - $string^1$ -> [$token_1^1$,$token_2^1$ ... $token_n^1$],
 - $string^2$ -> ==[$BOS$]== + [$token_1^2$,$token_2^2$ ... $token_m^2$]
2. **è¯åµŒå…¥**ï¼šä¸¤ç«¯æ–‡æœ¬çš„åˆ†è¯ç»“æœï¼Œåˆ†åˆ«ä½¿ç”¨å‘é‡è¡¨è¾¾
 - [$token_1^1$,$token_2^1$ ... $token_n^1$] -> $[x_1,x_2...x_n]$, 
 - ==[$BOS$]== + [$token_1^2$,$token_2^2$ ... $token_m^2$] -> $[y_{bos},y_1,y_2...y_m]$
3. **ä½ç½®ç¼–ç **ï¼šè¯åµŒå…¥+ä½ç½®ç¼–ç : 
 - $[x_1,x_2...x_n] + [pe_1, pe_2 ... pe_n]$ 
 - $[[BOS],y_1,y_2...y_m] + [pe_1, pe_2 ... pe_m,pe_{m+1}]$ 
4. **Encoder**: seq1 è¾“å…¥ç»™Encoderè¿›è¡Œè®¡ç®—
 - $[x_1,x_2...x_n] + [pe_1, pe_2 ... pe_n]$ -> $[z_1, z_2 ... z_n]$
5. **Decoder**: seq2å’ŒEncoderç»“æœï¼Œè¾“å‡ºé¢„æµ‹ç»“æœ: 
 - $[z_1, z_2 ... z_n]$ ,  $[[BOS],y_1,y_2...y_m] + [pe_1, pe_2 ... pe_m,pe_{m+1}]$  -> $[\hat{y_1},\hat{y_2}...\hat{y_m},[EOS]]$
6. è®¡ç®—loss

**æ¨ç†æµç¨‹ï¼š**
1. **åˆ†è¯**ï¼šå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯: string -> [$token_1$,$token_2$ ... $token_n$]
2. **è¯åµŒå…¥**ï¼šæ¯ä¸ªè¯ä½¿ç”¨å‘é‡è¡¨è¾¾: $[token_1,token_2 ... token_n]$ -> $[x_1,x_2 ... x_n]$
3. **ä½ç½®ç¼–ç **ï¼šæ¯ä¸ªè¯çš„è¯åµŒå…¥+å…¶ä½ç½®ç¼–ç : $[x_1,x_2 ... x_n] + [pe_1, pe_2 ... pe_n]$ -> $[\hat{x}_1, \hat{x}_2... \hat{x}_n]$
4. **Encoder**: è¾“å…¥ç»™Encoderè¿›è¡Œè®¡ç®—: $[\hat{x}_1, \hat{x}_2... \hat{x}_n]$ -> $[z_1, z_2 ... z_n]$
5. **Decoder**: Encoderè®¡ç®—ç»“æœå†ä¼ ç»™Decoder: $[z_1, z_2 ... z_n]$ -> $y_1$
6. **Decoder**: Encoder+Decoderç»“æœç»§ç»­ä¼ ç»™Decoder: $[z_1, z_2 ... z_n,y_1]$ -> $y_2$
7. é‡å¤ç›´è‡³ç»“æŸ

## 2. Transformerçš„è¾“å…¥

### 2.1 è¯åµŒå…¥
è¯åµŒå…¥çš„æ–¹å¼æœ‰å¾ˆå¤šï¼Œå¯ä»¥æ˜¯word2vecã€Gloveç­‰ç®—æ³•è®­ç»ƒå¾—åˆ°ï¼Œä¹Ÿå¯ä»¥æ˜¯**learnable**ï¼Œä¸transformerä¸€åŒè®­ç»ƒå¾—åˆ°

åœ¨è®ºæ–‡ä¸­ï¼Œword embeddingè¿˜ä¹˜ä»¥äº†$\sqrt{d}$

::: tip å¸¸è§é¢è¯•é¢˜
:::

::: details é—®é¢˜1. ä¸ºä»€ä¹ˆä¹˜ä»¥$\sqrt{d}$
:::

### 2.2 ä½ç½®ç¼–ç  ğŸ’–
æ ‡å‡†çš„ä½ç½®ç¼–ç æ˜¯

::: center
$PE_{(pos,2i)}=sin(pos / 1000^{2i/d})$

$PE_{(pos,2i+1)}=cos(pos / 1000^{2i/d})$
:::

å…¶ä¸­,$pos$ä¸º==å¥ä¸­çš„ä½ç½®==ï¼Œ$d$è¡¨ç¤º==ä½ç½®åµŒå…¥çš„ç»´åº¦==ï¼ˆç­‰äº==è¯åµŒå…¥çš„ç»´åº¦==ï¼‰ï¼Œ
$2i$è¡¨ç¤ºä½ç½®åµŒå…¥çš„==å¶æ•°ç»´åº¦==ï¼Œ$2i+1$è¡¨ç¤º==å¥‡æ•°ç»´åº¦==ï¼Œä¸”$2i<d$,$2i+1<d$

é™¤äº†fixedæ–¹å¼ï¼Œä¹Ÿå¯ä»¥é‡‡ç”¨learnableçš„ä½ç½®ç¼–ç ã€‚åœ¨åŸæ–‡ä¸­==æ•ˆæœä¸€æ ·==
ä½†æœ€ç»ˆè¿˜æ˜¯é€‰æ‹©äº†fixedæ–¹å¼ï¼Œå› ä¸ºè¿™ç§æ–¹æ³•èƒ½å¤Ÿ==å¤–æ¨åºåˆ—é•¿åº¦==ï¼Œä½¿å…¶èƒ½å¤„ç†==è¶…è¿‡è®­ç»ƒè¿‡ç¨‹çš„æœ€é•¿åºåˆ—==

::: tip å¸¸è§é¢è¯•é¢˜
:::

::: details é—®é¢˜1. transformerä¸ºä»€ä¹ˆè¦ä½¿ç”¨ä½ç½®ç¼–ç ï¼Ÿ
:::

::: details é—®é¢˜2. è®¾è®¡ä½¿ç”¨sinã€cosçš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ
å›ç­”ï¼š
æ›´å®¹æ˜“å­¦ä¹ åˆ°==ç›¸å¯¹ä½ç½®==çš„è¡¨è¾¾
:::

::: details é—®é¢˜3. è¿™ç§ä½ç½®ç¼–ç çš„å¥½å¤„å’Œåå¤„åˆ†åˆ«æœ‰ä»€ä¹ˆï¼Ÿ
:::
 

## 3. Transformerçš„æ¶æ„ 

### 3.1 Scaled Dot-Product Attetion ğŸ’–
queryå’Œkeyæ˜¯åŒä¸€å¤§å°çš„ç»´åº¦$d_k$ï¼Œvalueçš„ç»´åº¦æ˜¯$d_v$

::: center
![scaled_dot_product_attetion.png](/images/scaled_dot_product_attetion.png)
:::

**è®¡ç®—å…¬å¼**ï¼š

::: center
$Attention(Q,K,V) = softmax(\frac{Q*K^T}{\sqrt{d_k}})*V$
:::
å…¶ä¸­ï¼Œ$Q$çš„ç»´åº¦ä¸º$n \times d_k$, $K$çš„ç»´åº¦ä¸º$m \times d_k$ï¼Œ$V$çš„ç»´åº¦ä¸º$m \times d_v$


::: tip å¸¸è§é¢è¯•é¢˜
:::

::: details é—®é¢˜1. ä¸ºä»€ä¹ˆè¦é™¤ä»¥$\sqrt{d_k}$
å›ç­”ï¼š
å½“$d_k$çš„ç»´åº¦ç‰¹åˆ«å¤§æ—¶ï¼Œ$q*k$çš„å€¼ä¼šå˜å¾—ç‰¹åˆ«å¤§ï¼Œå¯¼è‡´softmaxè¾“å…¥è¶‹è¿‘äºä¸¤ç«¯ï¼Œä¼šå‡ºç°==æ¢¯åº¦æ¶ˆå¤±==
:::

### 3.2 å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶(Multi-Head Attention)  ğŸ’–
**åŠ¨æœº**ï¼šç±»æ¯”CNNçš„å¤šé€šé“ï¼Œèƒ½å¤Ÿ==æ•è·å¤šä¸ªä¸ä¸€æ ·çš„æ¨¡å¼==ï¼Œå®ç°å¤šä¸ªè¾“å‡ºé€šé“

::: center
![img.png](/images/MHA.png)
:::

QKVè¿›è¡Œhæ¬¡ä¸åŒçš„æŠ•å°„($d_?$->$d_?/h$)ï¼Œäº§ç”Ÿ$Q_i,K_i,V_i$(i=1...h)ã€‚è®¡ç®—è‡ªæ³¨æ„åŠ›ä¹‹åçš„ç»“æœï¼Œè¿›è¡Œ==æ‹¼æ¥==ã€‚


**è®¡ç®—å…¬å¼**ï¼š

::: center
$Multi-Head(Q,K,V) = concat(head_1,head_2,â€¦â€¦,head_h)*W^O$

$head_i = Attention(Q*W^Q_i,K*W^K_i,V*W^V_i)$

Qçš„ç»´åº¦$n,d_k$,Kçš„ç»´åº¦$m,d_k$,Vçš„ç»´åº¦æ˜¯$m,d_v$

$W^Q_i \in R^{\{d_k,d_k/h\}}$ï¼Œ$W^K_i \in R^{\{d_k,d_k/h\}}$ï¼Œ$W^V_i \in R^{\{d_v,d_v/h\}}$
$W^O_i \in R^{\{d_v,d_v\}}$
:::

### 3.3 Point-wise FeedForward Networks

==ä½•ä¸ºPoint-wiseï¼Ÿ==

å•ç‹¬ä½œç”¨åœ¨==è¾“å…¥çš„æ¯ä¸ªposition==ä¸Šï¼Œ==å¹¶éé’ˆå¯¹æ•´ä¸ªè¾“å…¥==ã€‚æ¯”å¦‚ï¼šè¾“å…¥æ˜¯ä¸€ä¸ª$n \times d_v$ï¼Œä½œç”¨åœ¨æ¯ä¸ª$1 \times d_v$ã€‚

::: center
 <img src="/images/FFN.png" style="zoom:30%" alt="Point-wise FeedForward Networks"  />
:::

**è®¡ç®—å…¬å¼ï¼š**

::: center
$FFN(\hat{x}) = Linear(ReLU(Linear(x)))$
:::

ç¬¬ä¸€ä¸ª$Linear \in R{\{d_{model},d_{model} * 4\}}$
ç¬¬äºŒä¸ª$Linear \in R{\{d_{model} * 4,d_{model}\}}$


::: tip å¸¸è§é¢è¯•é—®é¢˜
:::

::: details é—®é¢˜1ï¼šFFNä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿå¯ä»¥å»æ‰å—ï¼Ÿ
ä½œç”¨ï¼š
- ==ç‰¹å¾å¢å¼º==ï¼šç»è¿‡attentionåçš„ç»“æœï¼Œæ¯ä¸ªä½ç½®ä¸Šçš„å†…å®¹éƒ½æ˜¯å¯¹æ•´ä¸ªåºåˆ—çš„aggregationï¼Œä½¿ç”¨FFNæ¥å®Œæˆè¡¨å¾çš„å¢å¼º
- ==å¼•å…¥éçº¿æ€§==ï¼šVçš„æŠ•å½±æ˜¯çº¿æ€§çš„ã€‚attentionè®¡ç®—æ˜¯å¯¹Vçš„çº¿æ€§åŠ æƒå’Œï¼Œä¹Ÿæ˜¯çº¿æ€§çš„ã€‚éœ€è¦ä¾èµ–FFNæ¥å¼•å…¥éçº¿æ€§ï¼Œæ‹Ÿåˆæ›´å¤æ‚çš„æ¨¡å¼ã€‚
- ==çŸ¥è¯†å­˜å‚¨==: [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913)
:::

::: details é—®é¢˜2ï¼šFFNæœ‰ä»€ä¹ˆæ”¹è¿›å·¥ä½œï¼Ÿ
æ¿€æ´»å‡½æ•°ï¼š

Linearï¼š

:::

### è®¡ç®—å¤æ‚åº¦


::: tip å¸¸è§é¢è¯•é¢˜ç›®
:::

::: details é—®é¢˜1ï¼šã€ŠAttention is All Your Neddã€‹ä¸­çš„ä¸€äº›è¶…å‚æ•°è®¾ç½®
- Encoderå’ŒDecoderæ˜¯å¤šå°‘å±‚ï¼Ÿ ---- å‡ä¸º==6å±‚==
- æ¨¡å‹ä½¿ç”¨çš„æ¨¡å‹ç»´åº¦æ˜¯å¤šå°‘ï¼ŸFFNéšå±‚ä½¿ç”¨çš„ç»´åº¦ä½¿å¤šå°‘ï¼Ÿ ---- ==$d_model$512ç»´==ï¼Œ==$d_{ff}$æ˜¯2048ç»´
- ä½¿ç”¨çš„æ˜¯Pre-Normè¿˜æ˜¯Post-Norm? ---- ==Post-Norm==, $LayerNorm(x+SubLayer(x))$
- æ¨¡å‹ä½¿ç”¨çš„å¤´æ•°æ˜¯å¤šå°‘ï¼Ÿ ---- ==8ä¸ªå¤´==
:::