---
title: BatchNorm LayerNorm
tags:
  - batch norm
  - layer norm
  - basic
---

## BatchNorm vs. LayerNorm
### 什么是归一化？
对==神经元的输入==进行处理，使其符合==标准分布==

::: center
$ \bar{z} = \frac{z-\mu}{\sigma} $

$ \hat{z} = w * \bar{z} + b$

其中，$\mu$、$\sigma$分别为均值、标准差，$w$、$b$为可学习参数
:::

### 为什么需要归一化？

- **角度一**：==无量纲化==。==不同维度的值，可以统一使用==。比如：175cm和60kg特征取值返回、含义均不一致，无法直接运算
- **角度二**：==提高训练收敛效率==。
  - loss图上，正圆(归一化) 相对于 扁圆 收敛更快
  - 解决==梯度消失==。在使用sigmoid激活时，如果输入离0特别远，会出现梯度消失。落在0的附近，可有效缓解。
  - 解决==梯度爆炸==。对神经元参数($z=w*x+b$),求导数为$\frac{\partial z}{\partial w}  = x$，x的值如果特别大，累乘下去很容易出现梯度爆炸。
  - ==减小参数初始化的影响==。使模型不过分受参数初始化方法的影响
  - ==内部协向量偏移==。每一层的输入是上一层的输出，如果上一层的输出分布不固定，那么基于这个数据分布而训练往往会“不稳定”。层数越多，这种内部偏移会累加。
- **角度三**: ==训练和测试能对齐==
  - 能够使训练和测试阶段，各个神经元输入分布一致

::: note 回看
   ==角度一==并不是神经网络归一化的原因，这是==数据标准化==的操作
:::


| BN vs LN      |     BatchNorm     | LayerNorm |
|---------------|:-----------------:|----------:|
| 位置          |    一般是输出后，激活前     |     $1600 |
| col 2 is      | 训练时Batch Size不能太小 |       $12 |
| zebra stripes |     are neat      |        $1 |

### BatchNorm
沿某一feature维度进行归一化

### Layer
沿某一样本维度进行归一化